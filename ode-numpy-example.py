# -*- coding: utf-8 -*-
"""Копия блокнота "ODE 2 example.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o4gLCke_9H_oseL_B6fvNBOPrJ5mlSc8
"""

# Commented out IPython magic to ensure Python compatibility.
import autograd.numpy as np
from autograd import grad
import autograd.numpy.random as npr

from matplotlib import pyplot as plt
# %matplotlib inline

# Commented out IPython magic to ensure Python compatibility.
# %cd symbolic_linear_layer/

#TODO: 1. function to collect all variables from layer with or without values
#      2. add 2nd layer and check numpy derivative
#      3. Unite all this to a Model (Symbolic model) class with list of functions or layers in a chain

nx = 3
dx = 1. / nx

def f(x, psy, dpsy):
    '''
        d2(psy)/dx2 = f(x, dpsy/dx, psy)
        This is f() function on the right
    '''
    return -1./5. * np.exp(-x/5.) * np.cos(x) - 1./5. * dpsy - psy


def psy_analytic(x):
    '''
        Analytical solution of current problem
    '''
    return np.exp(-x/5.) * np.sin(x)

x_space = np.linspace(0, 2, nx)
y_space = psy_analytic(x_space)

def sigmoid(x):
    return 1. / (1. + np.exp(-x))

def neural_network(W, x):
    a1 = sigmoid(np.dot(x, W[0]))
    return np.dot(a1, W[1])

def neural_network_x(x):
    a1 = sigmoid(np.dot(x, W[0]))
    return np.dot(a1, W[1])

def psy_trial(xi, net_out):
    return xi + xi**2 * net_out

psy_grad = grad(psy_trial)
psy_grad2 = grad(psy_grad)

def loss_function(W, x):
    loss_sum = 0.

    for xi in x:
        net_out = neural_network(W, xi)[0][0]

        net_out_d = grad(neural_network_x)(xi)
        net_out_dd = grad(grad(neural_network_x))(xi)

        psy_t = psy_trial(xi, net_out)

        gradient_of_trial = psy_grad(xi, net_out)
        second_gradient_of_trial = psy_grad2(xi, net_out)

        func = f(xi, psy_t, gradient_of_trial) # right part function

        err_sqr = (second_gradient_of_trial - func)**2
        loss_sum += err_sqr

    return loss_sum

import torch
import torch.nn as nn

# class SymNN(nn.module):
#   def __init__(self,N):
#     super(SymNN).__init__()

if __name__ == '__main__':
    W = [npr.randn(1, nx), npr.randn(nx, 1)]
    lmb = 0.001

    x = np.ones(1)
    y = neural_network(W,x)
    y1 = np.dot(x, W[0])
    from symbolic_linear import SymbolicLinear,monkey_tensor,substitute_all_vars
    xt = monkey_tensor('x',torch.from_numpy(x))
    sfc = SymbolicLinear(1,len(W[0][0]))
    sfc.substitute_weight_and_biases(W[0][0],np.zeros(3))
    expr,s = sfc.symbolicEvaluate(xt,x)
    y2 = sigmoid(y1)
    from sym_sigmoid import SymbolicSigmoid
    y2_sim = SymbolicSigmoid(expr)

    from matrix_calculus import diff_expression_array,d_scalar_wrt_vector,d_vector_wrt_vector
    d_y2_d_x_0 = diff_expression_array(y2_sim,xt[0])
    d_y2_sim_d_w = d_scalar_wrt_vector(y2_sim[0],sfc.sym_weight)
    d_y2_sim_d_w = d_vector_wrt_vector(y2_sim, sfc.sym_weight)
    y2_sim_num = substitute_all_vars(y2_sim,
                                     [(xt,x),
                                      (sfc.sym_weight,sfc.weight.detach().numpy()),
                                      (sfc.sym_bias,sfc.bias.detach().numpy())])
    y3 = np.dot(y2, W[1])


    dy_dx = neural_network_x(np.ones(1))

    for i in range(50):
        loss_grad =  grad(loss_function)(W, x_space)

        W[0] = W[0] - lmb * loss_grad[0]
        W[1] = W[1] - lmb * loss_grad[1]

    print (loss_function(W, x_space))
    res = [psy_trial(xi, neural_network(W, xi)[0][0]) for xi in x_space]


    plt.figure()
    plt.plot(x_space, y_space)
    plt.plot(x_space, res)
    plt.show()





# Commented out IPython magic to ensure Python compatibility.
# %%bash
# git clone https://github.com/snytav/symbolic_linear_layer
#

